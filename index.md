---
title: Home
---

<h1> <span STYLE="font-size:25.0pt">G</span>ames <span STYLE="font-size:25.0pt">O</span>ptimization <span STYLE="font-size:25.0pt">A</span>lgorithms <span STYLE="font-size:25.0pt">L</span>earning </h1>

The Games, Optimization, Algorithms, and Learning Lab (GoalLab) studies theory of machine learning  and its interface with learning in games and algorithmic game theory, optimization, dynamical systems, probability and statistics.

# Research Highlights
{% capture text %}
Our recent focus has been on finding Nash equilibria in Markov games. Two representative papers include computing Nash Equilibria in [Adversarial Team Markov Games.](https://arxiv.org/abs/2208.02204) and [in Markov Potential Games.](https://arxiv.org/abs/2106.01969) Other works include analysis of [natural policy gradient in multi-agent settings](https://arxiv.org/abs/2110.10614).<br/>
{:.center}
{% endcapture %}
{%
  include feature.html
  image="images/papers/advteamgames.png"
  title="<strong> Multi-agent Reinforcement Learning </strong>"
  text= text
%}

{% capture text %}
The PI and the group is actively working on learning in games. Papers include results on last iterate convergence using optimism in [zero-sum games](https://arxiv.org/abs/1807.04252) and [beyond](https://arxiv.org/abs/2203.12056) (like potential games). Other works include proving [cycling](https://arxiv.org/abs/1710.11249) or even [chaotic behavior](https://arxiv.org/abs/1703.01138) of the learning dynamics, the analysis of [average performance](https://arxiv.org/abs/1403.3885) of learning in games and stability analysis in [evolutionary dynamics](https://arxiv.org/abs/1408.6270). 
{:.center}
{% endcapture %}
{%
  include feature.html
  image="images/lastiterate.png"
  title="<strong>Learning in Games</strong>"
  flip=true
  text=text
%}

{% capture text %}
Inspired by the success of Stochastic Gradient Descent in training neural networks, the group has done works on non-convex optimization. Using techniques from dynamical systems, we are able to show that Gradient Descent and other first order methods with constant stepsize [avoid strict saddle points](https://arxiv.org/abs/1710.07406). We extend these results to [multiplicative weights update (polytope constraints)](https://arxiv.org/abs/1810.05355) and [time varying stepsizes](https://arxiv.org/abs/1906.07772). Inpired by the success of Generative Adversarial Networks, the group has worked on min-max optimization for non-convex non-concave landscapes, [characterising the limit points of first-order methods](https://arxiv.org/abs/1807.03907).
{:.center}
{% endcapture %}
{%
  include feature.html
  image="images/nonconvex.png"
  title="<strong>Non-convex and min-max optimization</strong>"
  text=text
%}
{% capture text %}
The group has works related to [proper learning in Graphical models with applications to learning from dependent data](https://arxiv.org/abs/1905.03353) (see also [this paper](https://arxiv.org/abs/2003.08259) for a setting using hypergraphs), [structural learning from truncated data](https://arxiv.org/abs/2006.09735), [learning mixtures from truncated data](https://arxiv.org/abs/1902.06958). Other works are about [bounding the mixing time in Markov Chains](https://arxiv.org/abs/1411.6322) (see also [follow-up paper](https://panageas.github.io/files/dsmc.pdf)) 
{:.center}
{% endcapture %}
{%
  include feature.html
  image="images/trajectories.png"
  title="<strong>Probability and Statistics</strong>"
  flip=true
  text=text
%}

{% capture text %}
Our group has focused on the problem of expressivity of Neural Networks. Using techniques from Dynamical systems, we are able to prove [tradeoffs between the depth and the width in feedforward Neural Networks](https://arxiv.org/abs/1912.04378). Here is also a [follow-up paper](https://arxiv.org/abs/2003.00777) that strenghtens the results.
{:.center}
{% endcapture %}
{%
  include feature.html
  image="images/neuralmap.png"
  title="<strong>Deep Learning Theory</strong>"
  text=text
%}
# Our TEAM
{% capture text %}
Our team includes 7 PhD students, multiple undergrads and external collaborators.
{%
  include link.html
  link="team"
  icon="fas fa-arrow-right"
  text="Meet our team"
  flip=true
%}
{:.center}
{% endcapture %}
{%
  include feature.html
  image="images/team.png"
  link="team"
  text=text
%}

# <font color="red">Latest NEWS:</font>
  <ul>
    <li> 9/2024 One paper got accepted in NeurIPS 2024. </li>
    <li> 9/2024 Two papers got accepted in WINE 2024. </li>
    <li> 5/2024 One paper got accepted in ICML 2024. </li>
    <li> 4/2024 One paper got accepted in UAI 2024. </li>
    <li> 1/2024 Two papers got accepted in ICLR 2024. </li>
    <li> 12/2023 Two papers got accepted in AAAI 2024.</li>
    <li> 9/2023 Four papers got accepted in NeurIPS 2023.</li>
    <li> 5/2023 One <a href="https://arxiv.org/abs/2301.02129">
    paper</a> accepted in EC 2023. </li>
  <li> 4/2023 One paper accepted in ICML 2023 as oral. </li>
<li>2/2023: New paper on <a href="https://arxiv.org/abs/2301.11241">
        time-varying games.</a> </li>
<li>1/2023: Two papers accepted in ICLR 2023, <a href="https://arxiv.org/abs/2208.02204"> one oral.</a></li>
    </ul>
